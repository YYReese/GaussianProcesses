---
title: "Summer project"
author: "Ruizi Yan (s2265990)"
output:
  html_document:
    number_sections: yes
  pdf_document:
    number_sections: yes
header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
---

```{r setup, include = FALSE}
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.

# Set default code chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
theme_set(theme_bw())
#suppressPackageStartupMessages(library(matlib))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(mvtnorm))
# To give the same random number sequence every time the document is knit:ed,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
```

```{r code=readLines("functions.R"), eval=TRUE, echo=FALSE}
# Load function definitions
source("functions.R")
```
```{r load initializations, echo=FALSE}
# Load initialization data
load("initializations.RData")
```

Let $f$ be a function which is expensive to evaluated. We try to use Gaussian Process model to approximate the function $f$. To reduce the computational costs while achieving an acceptable accuracy, sequential design strategy is needed to choose samples to be observed. We will discuss two criteria in the project.


# Gaussian process functional regression

We consider the case with stationary noise. i.e.$\epsilon\sim \mathcal{N}(0,\sigma^2)$.

Let $f(x)$ be the objective function. Assume a prior distribution over the function $f(.)$. In particular we assume a zero-mean Gaussian process prior 
$$
f(.)\sim \mathcal{GP}(0,k(.,.))
$$
for some valid covariance function $k(.,.)$. Any covariance function $k(.,.)$ is valid provided that for any sets $X$ the resulting covariance matrix $K$ is positive semi-definite. One commonly used kernel is square exponential kernel, $k_{SE}(x,x')=\exp (-\frac 1 {2\tau^2}||x-x'||^2)$. For any $\tau >0$, $f(x)$ and $f(x')$ will have high covariance when $x$ and $x'$ are close. Lower covariance will be achieved if they are far apart. Matern kernels are also popular in many applications such as machine learning. In our example, we choose to use the 3/2 Matern kernel. A function `Matern_k` has been defined in the `functions.R` file which takes in two vector and return the squared exponential covariance matrix. 

Let $\mathbf{S}=\{(x^{(i)},y^{(i)}\}_{i=1}^m$ be an initial training set of i.i.d. samples and let $T=\{(x_*^{(i)},y_*^{(i)}\}_{i=1}^m$ be a set of i.i.d. testing points drawn from the same unknown distribution as $\mathbf{S}$. 

Define the notations: 

\[ X=
  \left[ {\begin{array}{ccc}
   - & (x^{(1)})^T & -\\
   - & (x^{(2)})^T & - \\
     & .           &\\
     & .           &\\
     & .           &\\
   - & (x^{(m)})^T & -
  \end{array} } \right] \in \mathbf{R}^{m\times n}
  , \vec{f}= 
  \left[ {\begin{array}{c}
  f^{(1)}\\
  f^{(2)}\\
  .\\
  .\\
  .\\
  f^{(m)}
  \end{array}} \right], 
  \vec{\epsilon} = \left[ 
  {\begin{array}{c}
  \epsilon^{(1)}\\
  \epsilon^{(2)}\\
  .\\
  .\\
  .\\
   \epsilon^{(m)}
  \end{array}} 
  \right],
  \vec{y} = \left[ 
  {\begin{array}{c}
  y^{(1)}\\
  y^{(2)}\\
  .\\
  .\\
  .\\
  y^{(m)}
  \end{array}}
  \right] \in \mathbf{R}^m,
\] 
\[ X_*=
  \left[ {\begin{array}{ccc}
   - & (x_*^{(1)})^T & -\\
   - & (x_*^{(2)})^T & - \\
     & .           &\\
     & .           &\\
     & .           &\\
   - & (x_*^{(m_*)})^T & -
  \end{array} } \right] \in \mathbf{R}^{m_*\times n}
  , \vec{f_*}= 
  \left[ {\begin{array}{c}
  f_*^{(1)}\\
  f_*^{(2)}\\
  .\\
  .\\
  .\\
  f_*^{(m_*)}
  \end{array}} \right], 
  \vec{\epsilon} = \left[ 
  {\begin{array}{c}
  \epsilon_*^{(1)}\\
  \epsilon_*^{(2)}\\
  .\\
  .\\
  .\\
   \epsilon_*^{(m_*)}
  \end{array}} 
  \right],
  \vec{y} = \left[ 
  {\begin{array}{c}
  y_*^{(1)}\\
  y_*^{(2)}\\
  .\\
  .\\
  .\\
  y_*^{(m_*)}
  \end{array}}
  \right] \in \mathbf{R}^{m_*}.
\]


For any function $\tilde{f}(.)$ drawn from a zero-mean GP prior with covariance function $k(.,.)$, the marginal distribution over any set of input points must have a joint multivariate Gaussian distribution. In particular it must hold for the training and testing points. Therefore 
\[
  \left[ {\begin{array}{cc}
    \vec f \\
    \vec {f_*} \\
  \end{array} } \right] \Bigg| X,X_*
  \sim \mathcal{N} \left( 0,
  \left[ {\begin{array}{cc}
  K(X,X) & K(X,X_*)\\
  K(X_*,X) & K(X_*,X_*)
  \end{array}} \right] \right)
\].

Recall the assumption that observation noises are from Gaussian distribution, $\vec \epsilon \sim \mathcal{N}(0,\sigma^2)$. The sum of two independent Gaussian random variables is also Gaussian. Therefore, 
\[
  \left[ {\begin{array}{cc}
    \vec y \\
    \vec {y_*} \\
  \end{array} } \right] \Bigg| X,X_* 
  = \left[ {\begin{array}{cc}
    \vec \epsilon \\
    \vec 0 \\
  \end{array} } \right]
  +
  \left[ {\begin{array}{cc}
    \vec f \\
    \vec {f_*} \\
  \end{array} } \right]
  \sim \mathcal{N} \left( 0,
  \left[ {\begin{array}{cc}
  K(X,X)+\sigma^2I & K(X,X_*)\\
  K(X_*,X) & K(X_*,X_*)
  \end{array}} \right] \right)
\]


Using the rules for conditional Gaussian, it follows that
$$
\vec {y}_*|\vec{y},X,X_* \sim \mathcal{N}(\mu^*,\Sigma^*)
$$
where 
$$
\mu^*=K(X_*,X)(K(X,X)+\sigma^2I)^{-1}\vec {y}
$$
$$
\Sigma^*=K(X_*,X_*)-K(X_*,X)(K(X,X)+\sigma^2I)^{-1}K(X,X_*).
$$

# Sequential design of experiments

## Criterion 1 -- Maximum Mean Squared Error (MMSE)
The first sample criterion chooses the next point where the uncertainty is the largest. 
$$
x_{new}=\arg \sup _{x\in \mathbb{R}} \{ \text {ESE}(f(x)|S)-\text{ESE}(f(x)|S\cup\{x\})\}.
$$
Here, $S$ is the set of points sampled so far.

Once a point $x$ is chosen, $f(x)|S\cup\{x\}=f(x)$, so $ESE(f(x)|S\cup\{x\})=0$. Then consider the expected squared error conditioned on $S$.


\begin{align*}
    \text{ESE}(f(x)|S)&=\mathbb{E}[(\tilde{f}(x)-f(x)]^2 \\
    &= \mathbb{E}[(\tilde{f}(x)-m(x)+m(x)-f(x))^2] \\
    &= \mathbb{E}[(\tilde{f}(x)-m(x))^2] + \mathbb{E}[(m(x)-f(x))^2] + \mathbb{E}[(\tilde{f}(x)-m(x)) (m(x)-f(x))]\\
    &= \mathbb{E}[(\tilde{f}(x)-m(x))^2] +(m(x)-f(x))^2\\
    &= \sigma^2(x)+(m(x)-f(x))^2
\end{align*}


For any $x$, $m(x)-f(x)$ is deterministic which we can do nothing about, so we concentrate on the first term $\sigma^2(x)$. Therefore the problem becomes finding the point where $\sigma^2(x)$ is maximized.

Noise-free 1d example of using MMSE, where initial desgin is set to be `r x[100]` and 101 observations in total.
```{r, echo=FALSE}
plt_pred(seq_DoE(x,y,c(100),criterion_1))
```
Stationary noise 1d example of using MMSE($\epsilon\sim \mathcal{N}(0,0.01)$), where initial desgin is set to be -4 and 101 observations in total.
```{r, echo=FALSE}
plt_pred(seq_DoE(x,y,c(100),criterion_1,sigma2_noise = 0.1^2))
```

## Criterion 2 -- Integrated Mean Squared Error (IMSE)

MMSE criterion is to choose the next point where the variance is the largest. However, it is not guaranteed that taking a sample at that point will reduce the overall uncertainty the most. To achieve this, we consider the following criterion:

$$
\arg \max_{x \in \mathbf{X}}[\int_X\text{Var}(f(y)|\mathbf{S})dy-\int_X\text{Var}(f(y)|\mathbf{S}\cup \{x\})dy]
$$

Unlike MMSE criterion which aims at reducing the pointwise uncertainty, IMSE criterion tries to reduce the overall uncertainty by the taking integrals.This quantity can be further simplified like the following.

$$
\begin{align*}
    &\int_X\text{Var}(f(y)|\mathbf{S})dy-\int_X\text{Var}(f(y)|\mathbf{S}\cup \{x\})dy \\
    & \sim \sum_y \text{Var}(f(y)|\mathbf{S})-\sum_y\text{Var}(f(y)|\mathbf{S}\cup \{x\})dy\\
    &= \text{tr} \Sigma_{.,.}-\text{tr} \Sigma_{.,.}^{\text{new}} \\
    &= \text{tr} \Sigma_{.,.}-(\text{tr}\Sigma_{.,.} -\text{tr}(\Sigma_{.,x}\Sigma_{x,x}^{-1} \Sigma_{x,.} )   \\
    &= \text{tr}(\Sigma_{x,.}\Sigma_{.,x}\Sigma_{x,x}) \\
    &= ||\Sigma_{x,.}||^2 \Sigma_{x,x}^{-1}
\end{align*}
$$


Noise-free 1d example of using IMSE, where initial desgin is set to be `r x[100]` and 101 observations in total.
```{r, echo=FALSE}
plt_pred(seq_DoE(x,y,c(100),criterion_2))
```

Stationary noise 1d example of using IMSE($\epsilon\sim \mathcal{N}(0,0.01)$), where initial desgin is set to be `r x[100]` and 101 observations in total.
```{r, echo=FALSE}
plt_pred(seq_DoE(x,y,c(100),criterion_1,sigma2_noise = 0.1^2))
```

## Criterion 3 -- Maximum Entropy (ME)



## Discussions on the two criteria

In this part, we discuss the performance of the model under two criteria. 

First, let the initial design to be the same, a randomly chosen point. We are going to investigate which criterion will give less uncertainty given the same iterations.

This is the plot of the total variance (computed by variance integral) and time cost of the two methods. The x axis is the total iterations performed.

```{r, echo=FALSE}
table1 <- readRDS(file = "data/table1.rds")
```

```{r,echo=FALSE}

ggplot(table1) +
  geom_line(aes(x=max_iterations, y=variances_1, col="Variances (criterion 1)")) +
  geom_line(aes(x=max_iterations, y=variances_2, col="Variances (criterion 2)")) +
  geom_line(aes(x=max_iterations, y=time_cost_1, col="Time cost (criterion 1)")) +
  geom_line(aes(x=max_iterations, y=time_cost_2, col="Time cost (criterion 2)")) +
  xlab("iterations") +
  ylab("Variances/Time cost")
```

We can see from the above figure that when the max iteration is small, criterion 2 has less overall variance than  criterion 1. However, as max iteration increases, the difference between the two decreases. Notice that when we do 20 iterations, the difference of the total variance between the two has already decreased to a negligible figure. It can therefore be inferred that the difference of the total variance will converge as max iteration goes up. With a large observation set, two criteria will result in almost the same results but criterion 1 costs less. To conclude, criterion 1 is favoured when around more than 15 iterations available, where we can have almost the same variance while costing less. If only 5 iterations (or less) are available, criterion 1 is preferable where we can achieve less variances with roughly the same computational cost.

To see how the two criteria reduce the prediction variance by sequentially getting new observations, the plots of the prediction variance after each update are presented here. The integers in grey boxes indicate the iteration state.
```{r,echo=FALSE}
var_dat1 <- readRDS(file = "data/var_dat1.rds")
var_dat1 %>%
  ggplot() +
  geom_line(aes(x,var)) +
  xlab("x") +
  ylab("variance") +
  ggtitle("Prediction variance plot (MMSE)") +
  facet_grid(rows = vars(iterations),scales = "free_y")
```

```{r, echo=FALSE}
var_dat2 <- readRDS(file = "data/var_dat2.rds")
var_dat2 %>%
  ggplot() +
  geom_line(aes(x,var)) +
  xlab("x") +
  ylab("variance") +
  ggtitle("Prediction variance plot (IMSE)") +
  facet_grid(rows = vars(iterations),scales = "free_y")
```

Clearly, MMSE chooses the next design which has the largest variance. Starting with one initial observation, the variance at the neighbours of the observed point is lower than that at the points far apart from it. If there are two observations, the maximum variance will  either be attained at the edges or at the midle of the two observed locations, depending on the distances. If the maximum distance between one observed point and the edge is greater than two times the distance of the two observed points themselves, that endpoint will be chosen to observe for next iteration. Otherwise it chooses the midpoint of the two observed points. In general, MMSE tends to spread points out, dragging down the peak one by one to a lower level unitl all the "peaks" are in the same level, and repeating. Due to this nature, the endpoints are more likely to be chosen at an early stage. For above illustration, the right endpoint is chosen just at the first iteration. 

On the contrary, IMSE aims to reduce the overall variance, "the area under the curve". Quite differently, dragging the edge down will never be the optimal choice for reducing the area under the curve". It can be seen from the above figure that the endpints have never be chosen over the 10 iterations. Even worse, in each "cycle of reduction", points near the edge are the last to be considered. 

This motivates us to investigate the case where we force the two endpoints to be observed. 
Choosing the initial design to be at the two endpoints, we are interested in how differently the two criteria make their decisions. 

```{r, echo=FALSE}
var_dat3 %>%
  ggplot() +
  geom_line(aes(x,var)) +
  xlab("x") +
  ylab("variance") +
  ggtitle("Prediction variance plot (MMSE)") +
  facet_grid(rows = vars(iterations),scales = "free_y")
```

```{r, echo=FALSE}
var_dat4 %>%
  ggplot() +
  geom_line(aes(x,var)) +
  xlab("x") +
  ylab("variance") +
  ggtitle("Prediction variance plot (IMSE") +
  facet_grid(rows = vars(iterations),scales = "free_y")
```

For the first few iterations, the two criteria make identical decisions. Although later on they choose different locations, the reduced variances are roughly the same. The following table summarises the total prediction variance after 5, 10, 20 and 30 iterations, where we can see the two criteria do result in the same total variance at two significant figure.
```{r,echo=FALSE}
table2 <- readRDS(file = "data/table2.rds")

knitr::kable(table2)
```


## Space-filling design vs sequential design

To see how sequential design benefits the computer experiments, we compare the results of space-filling strategy with that of the sequential strategy given the number of observations fixed to 20.

```{r}
# Space-filling
space_filling <- seq(1,length(x),len=20)
obs <- rep(FALSE, length(y))
x_idx_new <- space_filling
obs[x_idx_new] <- TRUE
y_hat <- rep(NA,length(y))

mu <- rep(0,length(y))
Sigma <- Matern_k(x,x)
Sigma_new <- Sigma

Sigma_new[obs, ] <- 0.0
Sigma_new[, obs] <- 0.0
  
# Update unobserved covariances and expectations:
Sigma_new[!obs, !obs] <- Sigma[!obs, !obs] -
    Sigma[!obs, obs,drop=FALSE] %*% 
    solve(Sigma[obs, obs, drop=FALSE], Sigma[obs, !obs,drop=FALSE])
  

y_hat[!obs] <-  mu[!obs] +
    Sigma[!obs, obs,drop=FALSE] %*%
    solve(Sigma[obs,obs,drop=FALSE], y[obs] - mu[obs])
y_hat[obs] <- y[obs]

Sigma <- Sigma_new

sf <- data.frame(x=x, y=y, y_hat=y_hat, var=pmax(0,diag(Sigma)))
```
```{r,echo=FALSE}
plt_space_filling <- ggplot(sf)+
  geom_ribbon(aes(x = x, ymin = y_hat-1.97*sqrt(var), ymax = y_hat+1.97*sqrt(var)),
              alpha = 0.2) +
  geom_line(aes(x = x, y = y_hat), linetype = "dashed") +
  geom_point(data = data.frame(x=x[obs],y=y[obs]),
             aes(x = x, y = y),size = 2) +
  theme_bw() +
  geom_line(aes(x=x,y=y), colour= "red")+
  ggtitle("Fitted GP model (Space_filling)")
```




```{r}
seq <- matrix(NA,nrow=length(space_filling),ncol=2)
for (i in 1:length(space_filling)){
  initial_design <- space_filling[seq(1,length(space_filling),len=i)]
  
  c1 <- seq_DoE(x,y,initial_design, criterion_1,max_iterations = 20-length(initial_design))
  seq[i,1] <- dx*sum(c1$var)
  c2 <- seq_DoE(x,y,initial_design, criterion_2,max_iterations = 20-length(initial_design))
  seq[i,2] <- dx*sum(c2$var)
}
```

```{r}
seq <- as.data.frame(seq)
colnames(seq) <- c("max_criterion","int_criterion")
ggplot(data=seq) +
  geom_line(aes(x=1:20,y=max_criterion,col="max_criterion")) +
  geom_line(aes(x=1:20,y=int_criterion,col="int_criterion")) +
  geom_hline(yintercept = dx*sum(sf$var),linetype="dashed") +
  ylab("Variance integral") +
  xlab("Number of initial (space-filling) designs")
```
The horizontal dash line indicates the variance integral given by 20 space-filling designs. This figure shows that if we start with 11 or less initial space-filling designs and sequentially choose the follow-up designs to make 20 observations in total, the variance integral will be greater than that given by purely space-filling designs. If the number of initial space-filling points is between 11 to 19, three methods give roughly the same total variances. If we make 19 initial space-filling designs and sequentially choose the last point, the total variances are less than that the purely sequential designs give. This proves that the sequential design strategies perform better than space-filling strategy in global fitting. 

Note that all the previous results are based on the assumption that we have the covariance function properly chosen, which is nearly unrealistic. To see how the covariance function matters and how much it can influence the outcome, we construct another function $g$ by the exponential kernel to test our models.
```{r}
y1 <- f(x)
y2 <- g(x)
```
```{r}
initial_design <- c(250,750)
f_km <- seq_DoE(x,y1,initial_design, criterion_1,Matern_k)

f_ke <- seq_DoE(x,y1,initial_design, criterion_2,Exp_k)

g_km <- seq_DoE(x,y2,initial_design, criterion_1,Matern_k)

g_ke <- seq_DoE(x,y2,initial_design, criterion_2,Exp_k)

```
```{r,echo=FALSE}
ggplot(f_km)+
  geom_ribbon(aes(x = x, ymin = y_hat-1.97*sqrt(var), ymax = y_hat+1.97*sqrt(var)),
              alpha = 0.2) +
  geom_line(aes(x = x, y = y_hat), linetype = "dashed") +
  geom_point(data = data.frame(x=x[f_km$obs],y=y1[f_km$obs]),
             aes(x = x, y = y),size = 2) +
  theme_bw() +
  geom_line(aes(x=x,y=y), colour= "red")+
  ggtitle("Fitted GP model (matern_kernel) for f")
```
```{r,echo=FALSE}
ggplot(f_ke)+
  geom_ribbon(aes(x = x, ymin = y_hat-1.97*sqrt(var), ymax = y_hat+1.97*sqrt(var)),
              alpha = 0.2) +
  geom_line(aes(x = x, y = y_hat), linetype = "dashed") +
  geom_point(data = data.frame(x=x[f_ke$obs],y=y1[f_ke$obs]),
             aes(x = x, y = y),size = 2) +
  theme_bw() +
  geom_line(aes(x=x,y=y), colour= "red")+
  ggtitle("Fitted GP model (exp_kernel) for f")
```

```{r,echo=FALSE}
ggplot(g_km)+
  geom_ribbon(aes(x = x, ymin = y_hat-1.97*sqrt(var), ymax = y_hat+1.97*sqrt(var)),
              alpha = 0.2) +
  geom_line(aes(x = x, y = y_hat), linetype = "dashed") +
  geom_point(data = data.frame(x=x[g_km$obs],y=y2[g_km$obs]),
             aes(x = x, y = y),size = 2) +
  theme_bw() +
  geom_line(aes(x=x,y=y), colour= "red")+
  ggtitle("Fitted GP model (matern_kernel) for g")
```
```{r,echo=FALSE}
ggplot(g_ke)+
  geom_ribbon(aes(x = x, ymin = y_hat-1.97*sqrt(var), ymax = y_hat+1.97*sqrt(var)),
              alpha = 0.2) +
  geom_line(aes(x = x, y = y_hat), linetype = "dashed") +
  geom_point(data = data.frame(x=x[g_ke$obs],y=y2[g_ke$obs]),
             aes(x = x, y = y),size = 2) +
  theme_bw() +
  geom_line(aes(x=x,y=y), colour= "red")+
  ggtitle("Fitted GP model (exp_kernel) for g")
```
Recall that $f$ is constructed by Matern kernel and $g$ is constructed by Exponential kernel. We apply our model with Exponential and Matern kernels respectively on $f$ and $g$. This table summarize the residual sum of squares (RSS) of the four combinations. For function $f$, the model using Matern kernel lead to a lower RSS. For function $g$, using Exponential kernel results in a lower RSS. We see that a kernel that matches the target function will make the surrogate model more desirable. Hence in practice we should make use of the prior knowledge as much as possible to choose a proper kernel to make the model more reliable. This becomes the main challenge, choosing a proper covariance function. In practice, we often have a guess about the graphical features of the target function. If the function tends to be rough, for example some signal functions in electronic engineering field, then one can choose exponential kernel or Matern kernel with small $\nu$. For smooth functions, the square exponential kernel may be applied. 
```{r,echo=FALSE}
fg_data <- data.frame(f=c(sum((f_ke$y_hat-y1)^2)*dx,sum((f_km$y_hat-y1)^2)*dx),
           g=c(sum((g_ke$y_hat-y2)^2)*dx,sum((g_km$y_hat-y2)^2)*dx))

rownames(fg_data) <- c("Exp_kernel","Mat_kernel")

knitr::kable(fg_data)
```






# Code appendix

```{r code=readLines("functions.R"), eval=FALSE, echo=TRUE}
```

```{r code=readLines("analysis.R"), eval=FALSE, echo=TRUE}
```

